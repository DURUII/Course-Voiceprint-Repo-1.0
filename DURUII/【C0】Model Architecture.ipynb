{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DURUII/HIMIA-course/blob/main/DURUII/%E3%80%90C0%E3%80%91Model%20Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.348181Z",
          "start_time": "2023-01-05T13:33:41.538235Z"
        },
        "id": "W5WU33KMdYVP"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "# 便于复现\n",
        "def same_seed(seed):\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "same_seed(2024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woYmlk0mdYVR"
      },
      "source": [
        "# 前端\n",
        "\n",
        "## X-VECTOR\n",
        "\n",
        "https://readpaper.com/paper/2890964092"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.364222Z",
          "start_time": "2023-01-05T13:33:42.350186Z"
        },
        "id": "mEbcqpYOdYVS"
      },
      "outputs": [],
      "source": [
        "class TDNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size=1,\n",
        "                 stride=1,\n",
        "                 padding=0,\n",
        "                 dilation=1,\n",
        "                 bias=False):\n",
        "        super(TDNN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv1d(in_channels,\n",
        "                              out_channels,\n",
        "                              kernel_size,\n",
        "                              stride,\n",
        "                              padding,\n",
        "                              dilation,\n",
        "                              bias=bias)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.bn(self.conv(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC3xJCfQdYVS"
      },
      "source": [
        "![](https://github.com/DURUII/HIMIA-course/blob/main/DURUII/res/gap.svg?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.380080Z",
          "start_time": "2023-01-05T13:33:42.365226Z"
        },
        "id": "9F2xpDXbdYVS"
      },
      "outputs": [],
      "source": [
        "class StatsPooling(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(StatsPooling, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        #  eg. conv1d - [batch, 1500, T]\n",
        "        #  eg. conv2d - [batch, 32*8, F, T]\n",
        "        x = x.view(x.shape[0], x.shape[1], -1)\n",
        "        mean = x.mean(dim=2)\n",
        "        std = x.std(dim=2)\n",
        "        return torch.cat([mean, std], dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5eoD1zFdYVS"
      },
      "source": [
        "![](https://pdf.cdn.readpaper.com/parsed/fetch_target/56f6fb62f8e0c0fa1dfdcfa47af86f1c_1_Figure_1.png)\n",
        "\n",
        "![](https://pdf.cdn.readpaper.com/parsed/fetch_target/a1d526b4bfc73adeb1746ccfc3654803_1_Table_1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.395218Z",
          "start_time": "2023-01-05T13:33:42.382084Z"
        },
        "id": "kPWdiLtedYVT"
      },
      "outputs": [],
      "source": [
        "class X_VECTOR(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim=24, hidden_dim=512, embedding_size=512):\n",
        "        super(X_VECTOR, self).__init__()\n",
        "        self.frame1 = TDNN(input_dim, hidden_dim, kernel_size=5, dilation=1)\n",
        "        self.frame2 = TDNN(hidden_dim, hidden_dim, kernel_size=3, dilation=2)\n",
        "        self.frame3 = TDNN(hidden_dim, hidden_dim, kernel_size=3, dilation=3)\n",
        "        self.frame4 = TDNN(hidden_dim, hidden_dim, kernel_size=1, dilation=1)\n",
        "        self.frame5 = TDNN(hidden_dim, 1500, kernel_size=1, dilation=1)\n",
        "        self.pool = StatsPooling()\n",
        "        self.fc1 = nn.Linear(1500 * 2, embedding_size)\n",
        "        self.fc2 = nn.Linear(embedding_size, embedding_size)\n",
        "\n",
        "    def forward(self, x, bottleneck_last=False):\n",
        "        print('Input Feature:', x.shape)\n",
        "        x = self.frame1(x)\n",
        "        print('Frame 1 Output:', x.shape)\n",
        "        x = self.frame2(x)\n",
        "        print('Frame 2 Output:', x.shape)\n",
        "        x = self.frame3(x)\n",
        "        print('Frame 3 Output:', x.shape)\n",
        "        x = self.frame4(x)\n",
        "        print('Frame 4 Output:', x.shape)\n",
        "        x = self.frame5(x)\n",
        "        print('Frame 5 Output:', x.shape)\n",
        "        x = self.pool(x)\n",
        "        print('Pooling Output:', x.shape)\n",
        "        embd_a = self.fc1(x)\n",
        "        print('FC1 Output:', embd_a.shape)\n",
        "        embd_b = self.fc2(embd_a)\n",
        "        print('FC2 Output:', embd_b.shape)\n",
        "        if bottleneck_last:\n",
        "            return embd_b\n",
        "        return embd_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.443327Z",
          "start_time": "2023-01-05T13:33:42.396205Z"
        },
        "id": "g5Ka9GLWdYVT",
        "outputId": "c5227f98-3fe8-4925-e339-386e8490980b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Feature: torch.Size([4, 24, 100])\n",
            "Frame 1 Output: torch.Size([4, 512, 96])\n",
            "Frame 2 Output: torch.Size([4, 512, 92])\n",
            "Frame 3 Output: torch.Size([4, 512, 86])\n",
            "Frame 4 Output: torch.Size([4, 512, 86])\n",
            "Frame 5 Output: torch.Size([4, 1500, 86])\n",
            "Pooling Output: torch.Size([4, 3000])\n",
            "FC1 Output: torch.Size([4, 512])\n",
            "FC2 Output: torch.Size([4, 512])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1943, -0.1423,  0.0615,  ...,  0.3088,  0.0225,  0.0509],\n",
              "        [ 0.2338, -0.1090,  0.1566,  ...,  0.2737,  0.0060,  0.2187],\n",
              "        [ 0.2274, -0.1500,  0.0751,  ...,  0.2390,  0.0536,  0.1253],\n",
              "        [ 0.1188, -0.1389,  0.1014,  ...,  0.2195,  0.0270,  0.1550]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "feats = torch.randn(4, 24, 100)\n",
        "model = X_VECTOR()\n",
        "model(feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9evMhwtydYVU"
      },
      "source": [
        "## ResNet34\n",
        "\n",
        "https://readpaper.com/paper/2949650786\n",
        "\n",
        "![](https://github.com/DURUII/HIMIA-course/blob/main/DURUII/res/resblock.svg?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.458366Z",
          "start_time": "2023-01-05T13:33:42.444329Z"
        },
        "id": "UGX-_xrRdYVU"
      },
      "outputs": [],
      "source": [
        "class BuildingBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, ConvLayer, NormLayer, in_planes, planes, stride=1):\n",
        "        super(BuildingBlock, self).__init__()\n",
        "        self.conv1 = ConvLayer(in_planes,\n",
        "                               planes,\n",
        "                               kernel_size=3,\n",
        "                               stride=stride,\n",
        "                               padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = NormLayer(planes)\n",
        "\n",
        "        self.conv2 = ConvLayer(planes,\n",
        "                               planes,\n",
        "                               kernel_size=3,\n",
        "                               stride=1,\n",
        "                               padding=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = NormLayer(planes)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # eg.  Input: torch.Size([4, 32, 40, 200])\n",
        "        # eg. Output: torch.Size([4, 64, 20, 100])\n",
        "        self.downsample = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.downsample = nn.Sequential(\n",
        "                ConvLayer(in_planes,\n",
        "                          self.expansion * planes,\n",
        "                          kernel_size=1,\n",
        "                          stride=stride,\n",
        "                          bias=False),\n",
        "                NormLayer(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #  print('Block Input:', x.shape)\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        x = self.downsample(x)\n",
        "        out += x\n",
        "        out = self.relu(out)\n",
        "        #  print('Block Output:', out.shape)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.474409Z",
          "start_time": "2023-01-05T13:33:42.459370Z"
        },
        "id": "yy8Zf_fpdYVU"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_planes,\n",
        "                 block,\n",
        "                 num_blocks,\n",
        "                 num_classes=10,\n",
        "                 in_ch=1,\n",
        "                 feat_dim='2d',\n",
        "                 **kwargs):\n",
        "        super(ResNet, self).__init__()\n",
        "        if feat_dim == '1d':\n",
        "            self.ConvLayer = nn.Conv1d\n",
        "            self.NormLayer = nn.BatchNorm1d\n",
        "        elif feat_dim == '2d':\n",
        "            self.ConvLayer = nn.Conv2d\n",
        "            self.NormLayer = nn.BatchNorm2d\n",
        "        elif feat_dim == '3d':\n",
        "            self.ConvLayer = nn.Conv3d\n",
        "            self.NormLayer = nn.BatchNorm3d\n",
        "        else:\n",
        "            print('error')\n",
        "\n",
        "        self.in_planes = in_planes\n",
        "        self.conv1 = self.ConvLayer(in_ch,\n",
        "                                    in_planes,\n",
        "                                    kernel_size=3,\n",
        "                                    stride=1,\n",
        "                                    padding=1,\n",
        "                                    bias=False)\n",
        "        self.bn1 = self.NormLayer(in_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block,\n",
        "                                       planes=in_planes,\n",
        "                                       num_blocks=num_blocks[0],\n",
        "                                       stride=1)\n",
        "        self.layer2 = self._make_layer(block,\n",
        "                                       planes=in_planes * 2,\n",
        "                                       num_blocks=num_blocks[1],\n",
        "                                       stride=2)\n",
        "        self.layer3 = self._make_layer(block,\n",
        "                                       planes=in_planes * 4,\n",
        "                                       num_blocks=num_blocks[2],\n",
        "                                       stride=2)\n",
        "        self.layer4 = self._make_layer(block,\n",
        "                                       planes=in_planes * 8,\n",
        "                                       num_blocks=num_blocks[3],\n",
        "                                       stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "\n",
        "        for stride in strides:\n",
        "            layers.append(\n",
        "                block(self.ConvLayer,\n",
        "                      self.NormLayer,\n",
        "                      self.in_planes,\n",
        "                      planes,\n",
        "                      stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut61_9c-dYVV"
      },
      "source": [
        "![](https://pdf.cdn.readpaper.com/parsed/fetch_target/27f2cdce86e93cfe09084cc4d1009835_1_Table_2_431817400.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.490451Z",
          "start_time": "2023-01-05T13:33:42.475411Z"
        },
        "id": "wiTlo3scdYVV"
      },
      "outputs": [],
      "source": [
        "class StatsPoolingFlatten(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(StatsPoolingFlatten, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        #  eg. input: [@batch, channels=256, H/F=5, W/T=T/8]\n",
        "        x = x.view(x.shape[0], x.shape[1] * x.shape[2], -1)\n",
        "        mean = x.mean(dim=2)\n",
        "        std = x.std(dim=2)\n",
        "        return torch.cat([mean, std], dim=1)\n",
        "\n",
        "\n",
        "class ResNet34StatsPooling(nn.Module):\n",
        "\n",
        "    def __init__(self, in_planes, embedding_size, dropout=0, **kwargs):\n",
        "        super(ResNet34StatsPooling, self).__init__()\n",
        "        self.front = ResNet(in_planes, BuildingBlock, [3, 4, 6, 3], **kwargs)\n",
        "        self.pool = StatsPoolingFlatten()  # [batch, 32*8=256, F/r, T/r]\n",
        "        self.bottleneck = nn.Linear(in_planes * 8 * 2 * 5, embedding_size)\n",
        "        self.drop = nn.Dropout(drop) if dropout else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(dim=1)\n",
        "        print('Input Size:', x.shape)\n",
        "        x = self.front(x)\n",
        "        print('ResNet Output:', x.shape)\n",
        "        x = self.pool(x)\n",
        "        print('Pooling Output:', x.shape)\n",
        "        x = self.bottleneck(x)\n",
        "        print('Embedding Output:', x.shape)\n",
        "\n",
        "        if self.drop:\n",
        "            x = self.drop(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.617674Z",
          "start_time": "2023-01-05T13:33:42.491453Z"
        },
        "id": "S0sGytjOdYVV",
        "outputId": "2740f89e-2ae9-4889-e2bc-822bd8cab7cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Size: torch.Size([4, 1, 40, 200])\n",
            "ResNet Output: torch.Size([4, 256, 5, 25])\n",
            "Pooling Output: torch.Size([4, 2560])\n",
            "Embedding Output: torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "model = ResNet34StatsPooling(in_planes=32, embedding_size=256, dropout=0)\n",
        "feats = torch.randn(4, 40, 200)\n",
        "embd = model(feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYPZUm_CdYVV"
      },
      "source": [
        "## SE-Module\n",
        "\n",
        "If a network can be enhanced from the aspect of channel relationship?\n",
        "——[ImageNet 冠军模型 SE-Net 详解](https://www.bilibili.com/video/BV1Up4y187qb/?share_source=copy_web&vd_source=ef1797dacc3337c5dbcd89d489cc72a3)\n",
        "\n",
        "https://readpaper.com/paper/2963420686\n",
        "\n",
        "\n",
        "\n",
        "![](https://pdf.cdn.readpaper.com/parsed/fetch_target/5e8a39a0a2107065a20f8092224863f0_1_Figure_1.png)\n",
        "\n",
        "![](https://pdf.cdn.readpaper.com/parsed/fetch_target/5e8a39a0a2107065a20f8092224863f0_3_Figure_3.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.633715Z",
          "start_time": "2023-01-05T13:33:42.619678Z"
        },
        "id": "FzoGec2QdYVW"
      },
      "outputs": [],
      "source": [
        "class SELayer(nn.Module):\n",
        "\n",
        "    def __init__(self, channel, reduction=8):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.sq = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        nn.ex = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _ = x.size()\n",
        "        y = self.sq(x).view(b, c)\n",
        "        y = self.ex(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.649757Z",
          "start_time": "2023-01-05T13:33:42.634718Z"
        },
        "id": "3PgnfE4EdYVW"
      },
      "outputs": [],
      "source": [
        "class SEBasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self,\n",
        "                 ConvLayer,\n",
        "                 NormLayer,\n",
        "                 in_planes,\n",
        "                 planes,\n",
        "                 stride=1,\n",
        "                 reduction=8):\n",
        "        super(SEBasicBlock, self).__init__()\n",
        "        self.conv1 = ConvLayer(in_planes,\n",
        "                               planes,\n",
        "                               kernel_size=3,\n",
        "                               stride=stride,\n",
        "                               padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = NormLayer(planes)\n",
        "        self.conv2 = ConvLayer(planes,\n",
        "                               planes,\n",
        "                               kernel_size=3,\n",
        "                               stride=1,\n",
        "                               padding=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = NormLayer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.se = SELayer(planes, reduction)\n",
        "\n",
        "        self.downsample = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.downsample = nn.Sequential(\n",
        "                ConvLayer(in_planes,\n",
        "                          self.expansion * planes,\n",
        "                          kernel_size=1,\n",
        "                          stride=stride,\n",
        "                          bias=False), NormLayer(self.expansion * planes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.se(out)\n",
        "\n",
        "        out += self.downsample(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SEResNet34StatsPooling(nn.Module):\n",
        "\n",
        "    def __init__(self, in_planes, embedding_size, dropout=0.5, **kwargs):\n",
        "        super(SEResNet34StatsPooling, self).__init__()\n",
        "        self.front = ResNet(in_planes, SEBasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "        self.pool = StatsPooling()\n",
        "        self.bottleneck = nn.Linear(in_planes * 8 * 2 * 5, embedding_size)\n",
        "        self.drop = nn.Dropout(drop) if dropout else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(dim=1)\n",
        "        print('Input Size:', x.shape)\n",
        "        x = self.front(x)\n",
        "        print('ResNet Output:', x.shape)\n",
        "        x = self.pool(x)\n",
        "        print('Pooling Output:', x.shape)\n",
        "        x = self.bottleneck(x)\n",
        "        print('Embedding Output:', x.shape)\n",
        "\n",
        "        if self.drop:\n",
        "            x = self.drop(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.777715Z",
          "start_time": "2023-01-05T13:33:42.650760Z"
        },
        "id": "cZhStufMdYVW",
        "outputId": "4790ccf0-572c-4127-83c9-998d0fcfaa39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Size: torch.Size([4, 1, 40, 200])\n",
            "ResNet Output: torch.Size([4, 256, 5, 25])\n",
            "Pooling Output: torch.Size([4, 2560])\n",
            "Embedding Output: torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "model = ResNet34StatsPooling(in_planes=32, embedding_size=256, dropout=0)\n",
        "feats = torch.randn(4, 40, 200)\n",
        "embd = model(feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An19Gu08dYVW"
      },
      "source": [
        "# 编码\n",
        "\n",
        "## ASP\n",
        "\n",
        "https://readpaper.com/paper/2794506738\n",
        "\n",
        "![](https://pdf.cdn.readpaper.com/parsed/fetch_target/56f6fb62f8e0c0fa1dfdcfa47af86f1c_2_Figure_2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.792754Z",
          "start_time": "2023-01-05T13:33:42.778717Z"
        },
        "id": "JTUjN_54dYVW"
      },
      "outputs": [],
      "source": [
        "class ASP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim=1500, bottlneck_dim=120):\n",
        "        super(ASP, self).__init__()\n",
        "        #  eg. Frame 5 Output: torch.Size([4, 1500, 86])\n",
        "        self.attention = nn.Sequential(\n",
        "            # Use Conv1d with stride == 1 rather than Linear, then we don't need to transpose inputs.\n",
        "            nn.Conv1d(input_dim, bottlneck_dim, kernel_size=1),\n",
        "            # DON'T use ReLU here! In experiments, ReLU is found hard to converge.\n",
        "            nn.Tanh(),\n",
        "            nn.BatchNorm1d(bottlneck_dim),\n",
        "            nn.Conv1d(bottlneck_dim, input_dim, kernel_size=1),\n",
        "            nn.Softmax(dim=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.attention(x)\n",
        "        mean = torch.sum(x * w, dim=2)\n",
        "\n",
        "        std = torch.sqrt((\n",
        "                                 torch.sum((x ** 2) * w, dim=2) - mean ** 2)\n",
        "                         .clamp(min=1e-5)  # min where x in less than min\n",
        "                         )\n",
        "\n",
        "        x = torch.cat((mean, std), 1)\n",
        "        return x.view(x.shape[0], -1)\n",
        "\n",
        "\n",
        "class X_VECTOR_ASP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim=24, hidden_dim=512, embedding_size=512):\n",
        "        super(X_VECTOR_ASP, self).__init__()\n",
        "        self.frame1 = TDNN(input_dim, hidden_dim, kernel_size=5, dilation=1)\n",
        "        self.frame2 = TDNN(hidden_dim, hidden_dim, kernel_size=3, dilation=2)\n",
        "        self.frame3 = TDNN(hidden_dim, hidden_dim, kernel_size=3, dilation=3)\n",
        "        self.frame4 = TDNN(hidden_dim, hidden_dim, kernel_size=1, dilation=1)\n",
        "        self.frame5 = TDNN(hidden_dim, 1500, kernel_size=1, dilation=1)\n",
        "        self.pool = ASP()\n",
        "        self.fc1 = nn.Linear(1500 * 2, embedding_size)\n",
        "        self.fc2 = nn.Linear(embedding_size, embedding_size)\n",
        "\n",
        "    def forward(self, x, bottleneck_last=False):\n",
        "        print('Input Feature:', x.shape)\n",
        "        x = self.frame1(x)\n",
        "        print('Frame 1 Output:', x.shape)\n",
        "        x = self.frame2(x)\n",
        "        print('Frame 2 Output:', x.shape)\n",
        "        x = self.frame3(x)\n",
        "        print('Frame 3 Output:', x.shape)\n",
        "        x = self.frame4(x)\n",
        "        print('Frame 4 Output:', x.shape)\n",
        "        x = self.frame5(x)\n",
        "        print('Frame 5 Output:', x.shape)\n",
        "        x = self.pool(x)\n",
        "        print('Pooling Output:', x.shape)\n",
        "        embd_a = self.fc1(x)\n",
        "        print('FC1 Output:', embd_a.shape)\n",
        "        embd_b = self.fc2(embd_a)\n",
        "        print('FC2 Output:', embd_b.shape)\n",
        "        if bottleneck_last:\n",
        "            return embd_b\n",
        "        return embd_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.856922Z",
          "start_time": "2023-01-05T13:33:42.793757Z"
        },
        "id": "9sY1gJigdYVW",
        "outputId": "3dae6aa6-567c-4594-8890-e87f309b51f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Feature: torch.Size([4, 24, 200])\n",
            "Frame 1 Output: torch.Size([4, 512, 196])\n",
            "Frame 2 Output: torch.Size([4, 512, 192])\n",
            "Frame 3 Output: torch.Size([4, 512, 186])\n",
            "Frame 4 Output: torch.Size([4, 512, 186])\n",
            "Frame 5 Output: torch.Size([4, 1500, 186])\n",
            "Pooling Output: torch.Size([4, 3000])\n",
            "FC1 Output: torch.Size([4, 512])\n",
            "FC2 Output: torch.Size([4, 512])\n"
          ]
        }
      ],
      "source": [
        "feats = torch.rand(4, 24, 200)\n",
        "model = X_VECTOR_ASP()\n",
        "embd = model(feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIhQ2f1ZdYVW"
      },
      "source": [
        "# 输出\n",
        "\n",
        "## ArcFace\n",
        "\n",
        "https://readpaper.com/paper/2784874046\n",
        "\n",
        "![](https://pdf.cdn.readpaper.com/parsed/fetch_target/e95210a97106b72ca72d1dac425ebb16_3_Figure_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Mvru-UdYVW"
      },
      "source": [
        "$$Softmax \\ Loss = -log \\frac{e^{W^T_{y_i} x_i+b_{y_i}}}{\\sum_{j=1}^{N} {e^{W^T_j x_i+b_j}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfrGggludYVX"
      },
      "source": [
        "$$Arcface \\ Loss = -log \\frac{e^{s(\\cos (\\theta_{y_i}+m))}}{e^{s(\\cos (\\theta_{y_i}+m))}+\\sum_{{j=1},j\\ne y_i}^{n} {e^{s\\cos \\theta_j}}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:42.872964Z",
          "start_time": "2023-01-05T13:33:42.857925Z"
        },
        "id": "Uog2SfOVdYVX"
      },
      "outputs": [],
      "source": [
        "class AAMSoftmax(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 out_features,\n",
        "                 device_id,\n",
        "                 s=30.0,\n",
        "                 m=0.50,\n",
        "                 easy_margin=False):\n",
        "        super(AAMSoftmax, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        self.device_id = device_id\n",
        "\n",
        "        # Xavier Initialization\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
        "        if self.device_id == None:\n",
        "            cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        else:\n",
        "            x = input\n",
        "            sub_weights = torch.chunk(self.weight, len(self.device_id), dim=0)\n",
        "            temp_x = x.cuda(self.device_id[0])\n",
        "            weight = sub_weights[0].cuda(self.device_id[0])\n",
        "            cosine = F.linear(F.normalize(temp_x), F.normalize(weight))\n",
        "            for i in range(1, len(self.device_id)):\n",
        "                temp_x = x.cuda(self.device_id[0])\n",
        "                weight = sub_weights[i].cuda(self.device_id[i])\n",
        "                cosine = torch.cat(\n",
        "                    (cosine,\n",
        "                     F.linear(F.normalize(temp_x),\n",
        "                              F.normalize(weight)).cuda(self.device_id[i])))\n",
        "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "\n",
        "        # --------------------------- convert label to one-hot ---------------------------\n",
        "        one_hot = torch.zeros(cosine.size())\n",
        "        if self.device_id != None:\n",
        "            one_hot = one_hot.cuda(self.device_id[0])\n",
        "        one_hot.scatter(1, label.view(-1, 1).long(), 1)\n",
        "\n",
        "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:46.926847Z",
          "start_time": "2023-01-05T13:33:42.873967Z"
        },
        "id": "s5xqE-cydYVX",
        "outputId": "9221bb7a-dd11-4487-ac17-ecbeb8095fc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Feature: torch.Size([4, 24, 200])\n",
            "Frame 1 Output: torch.Size([4, 512, 196])\n",
            "Frame 2 Output: torch.Size([4, 512, 192])\n",
            "Frame 3 Output: torch.Size([4, 512, 186])\n",
            "Frame 4 Output: torch.Size([4, 512, 186])\n",
            "Frame 5 Output: torch.Size([4, 1500, 186])\n",
            "Pooling Output: torch.Size([4, 3000])\n",
            "FC1 Output: torch.Size([4, 512])\n",
            "FC2 Output: torch.Size([4, 512])\n",
            "torch.Size([4, 1000])\n"
          ]
        }
      ],
      "source": [
        "feats = torch.rand(4, 24, 200).cuda()\n",
        "model = X_VECTOR_ASP().cuda()\n",
        "embd = model(feats).cuda()\n",
        "classifier = AAMSoftmax(512, 1000, device_id=[0], s=30.0, m=0.50)\n",
        "output = classifier(embd, torch.tensor([0, 1, 2, 3]).cuda())\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI0nYZmKdYVX"
      },
      "source": [
        "# 作业\n",
        "\n",
        "## SE-Res2Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:46.942889Z",
          "start_time": "2023-01-05T13:33:46.927851Z"
        },
        "pycharm": {
          "is_executing": true
        },
        "id": "2HKO53qldYVX"
      },
      "outputs": [],
      "source": [
        "class Conv1dReluBn(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size=1,\n",
        "                 stride=1,\n",
        "                 padding=0,\n",
        "                 dilation=1,\n",
        "                 bias=False):\n",
        "        super(Conv1dReluBn, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv1d(in_channels,\n",
        "                              out_channels,\n",
        "                              kernel_size,\n",
        "                              stride,\n",
        "                              padding,\n",
        "                              dilation,\n",
        "                              bias=bias)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "\n",
        "class Res2DilatedConv1dReluBn(nn.Module):\n",
        "    # padding and dilation shoule be carefully set equal\n",
        "    def __init__(self, channels, kernel_size, dilation, padding, scale=4):\n",
        "        super(Res2DilatedConv1dReluBn, self).__init__()\n",
        "\n",
        "        self.scale = scale\n",
        "\n",
        "        self.l = []\n",
        "        for i in range(2):\n",
        "            self.l.append(Conv1dReluBn(in_channels=channels, out_channels=channels, kernel_size=1,\n",
        "                                       stride=1, padding=0, dilation=1, bias=False))\n",
        "        self.l = nn.ModuleList(self.l)\n",
        "\n",
        "        self.k = []\n",
        "        for i in range(scale - 1):\n",
        "            self.k.append(\n",
        "                Conv1dReluBn(in_channels=channels // scale, out_channels=channels // scale, kernel_size=kernel_size,\n",
        "                             stride=1, padding=padding, dilation=dilation, bias=False))\n",
        "\n",
        "        self.k = nn.ModuleList(self.k)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l[0](x)\n",
        "        xs = torch.chunk(x, self.scale, dim=1)\n",
        "        ys = [i for i in range(self.scale)]\n",
        "        ys[0] = xs[0]\n",
        "        ys[1] = self.k[0](xs[1])\n",
        "        ys[2] = self.k[1](xs[2] + ys[1])\n",
        "        ys[3] = self.k[2](xs[3] + ys[2])\n",
        "        y = torch.cat(ys, dim=1)\n",
        "\n",
        "        y = self.l[1](y)\n",
        "        return y + x\n",
        "\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, channels, reduction=8):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.sq = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        self.ex = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _ = x.size()\n",
        "        y = self.sq(x).view(b, c)\n",
        "        y = self.ex(y).view(b, c, 1)\n",
        "        return x * y.expand_as(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:46.957928Z",
          "start_time": "2023-01-05T13:33:46.943892Z"
        },
        "pycharm": {
          "is_executing": true
        },
        "id": "LFeOgCx5dYVX"
      },
      "outputs": [],
      "source": [
        "class SERes2Block(nn.Module):\n",
        "    def __init__(self, channels, kernel_size, dilation, padding):\n",
        "        super(SERes2Block, self).__init__()\n",
        "        self.frame1 = Conv1dReluBn(channels, channels)\n",
        "        self.frame2 = Res2DilatedConv1dReluBn(channels, kernel_size, dilation, padding)\n",
        "        self.frame3 = Conv1dReluBn(channels, channels)\n",
        "        self.frame4 = SEBlock(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y1 = self.frame1(x)\n",
        "        y2 = self.frame2(y1)\n",
        "        y3 = self.frame3(y2)\n",
        "        y4 = self.frame4(y3)\n",
        "        return x + y4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:46.973971Z",
          "start_time": "2023-01-05T13:33:46.958932Z"
        },
        "pycharm": {
          "is_executing": true
        },
        "id": "EKdKlCr2dYVX"
      },
      "outputs": [],
      "source": [
        "class AAMSoftmax(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 out_features,\n",
        "                 device_id,\n",
        "                 s=30.0,\n",
        "                 m=0.50,\n",
        "                 easy_margin=False):\n",
        "        super(AAMSoftmax, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        self.device_id = device_id\n",
        "\n",
        "        # Xavier Initialization\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
        "        if self.device_id == None:\n",
        "            cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        else:\n",
        "            x = input\n",
        "            sub_weights = torch.chunk(self.weight, len(self.device_id), dim=0)\n",
        "            temp_x = x.cuda(self.device_id[0])\n",
        "            weight = sub_weights[0].cuda(self.device_id[0])\n",
        "            cosine = F.linear(F.normalize(temp_x), F.normalize(weight))\n",
        "            for i in range(1, len(self.device_id)):\n",
        "                temp_x = x.cuda(self.device_id[0])\n",
        "                weight = sub_weights[i].cuda(self.device_id[i])\n",
        "                cosine = torch.cat(\n",
        "                    (cosine,\n",
        "                     F.linear(F.normalize(temp_x),\n",
        "                              F.normalize(weight)).cuda(self.device_id[i])))\n",
        "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "\n",
        "        # --------------------------- convert label to one-hot ---------------------------\n",
        "        one_hot = torch.zeros(cosine.size())\n",
        "        if self.device_id != None:\n",
        "            one_hot = one_hot.cuda(self.device_id[0])\n",
        "        one_hot.scatter(1, label.view(-1, 1).long(), 1)\n",
        "\n",
        "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q93GggSWdYVX"
      },
      "source": [
        "## ECAPA-TDNN\n",
        "\n",
        "https://readpaper.com/paper/3024869864"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T02:22:18.266327Z",
          "start_time": "2023-01-05T02:22:18.119740Z"
        },
        "id": "LbVj20PudYVY"
      },
      "source": [
        "![](https://pdf.cdn.readpaper.com/parsed/fetch_target/7dcf7bfaa875a883299e72921f6137ac_2_Figure_2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:46.989010Z",
          "start_time": "2023-01-05T13:33:46.974974Z"
        },
        "pycharm": {
          "is_executing": true
        },
        "id": "e5Ec8jUodYVY"
      },
      "outputs": [],
      "source": [
        "class ASP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim=1500, bottlneck_dim=120):\n",
        "        super(ASP, self).__init__()\n",
        "        #  eg. Frame 5 Output: torch.Size([4, 1500, 86])\n",
        "        self.attention = nn.Sequential(\n",
        "            # Use Conv1d with stride == 1 rather than Linear, then we don't need to transpose inputs.\n",
        "            nn.Conv1d(input_dim, bottlneck_dim, kernel_size=1),\n",
        "            # DON'T use ReLU here! In experiments, ReLU is found hard to converge.\n",
        "            nn.Tanh(),\n",
        "            nn.BatchNorm1d(bottlneck_dim),\n",
        "            nn.Conv1d(bottlneck_dim, input_dim, kernel_size=1),\n",
        "            nn.Softmax(dim=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.attention(x)\n",
        "        mean = torch.sum(x * w, dim=2)\n",
        "\n",
        "        std = torch.sqrt((torch.sum((x ** 2) * w, dim=2) - mean ** 2)\n",
        "                         .clamp(min=1e-5)  # min where x in less than min\n",
        "                         )\n",
        "\n",
        "        x = torch.cat((mean, std), 1)\n",
        "        return x.view(x.shape[0], -1)\n",
        "\n",
        "\n",
        "class ECAPA_TDNN(nn.Module):\n",
        "    def __init__(self, F=80, C=512, E=192, S=10):\n",
        "        super(ECAPA_TDNN, self).__init__()\n",
        "        self.layer1 = Conv1dReluBn(F, C, kernel_size=5, padding=2)\n",
        "        self.layer2 = SERes2Block(C, 3, 2, 2)\n",
        "        self.layer3 = SERes2Block(C, 3, 3, 3)\n",
        "        self.layer4 = SERes2Block(C, 3, 4, 4)\n",
        "        self.layer5 = Conv1dReluBn(3 * C, 3 * C)\n",
        "        self.pool = ASP(input_dim=3 * C)\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Linear(3 * C * 2, E),\n",
        "            nn.BatchNorm1d(E)\n",
        "        )\n",
        "        # self.layer8 = AAMSoftmax(in_features=E, out_features=S, device_id=None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y1 = self.layer1(x)\n",
        "        y2 = y1 + self.layer2(y1)\n",
        "        y3 = y1 + y2 + self.layer3(y2)\n",
        "        y4 = y1 + y2 + y3 + self.layer4(y3)\n",
        "\n",
        "        x5 = torch.cat([y2, y3, y4], dim=1)\n",
        "        y5 = self.layer5(x5)\n",
        "\n",
        "        y6 = self.pool(y5)\n",
        "\n",
        "        y7 = self.layer7(y6)\n",
        "        # y8 = self.layer8(y7)\n",
        "\n",
        "        return y7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T13:33:47.133117Z",
          "start_time": "2023-01-05T13:33:46.990013Z"
        },
        "pycharm": {
          "is_executing": true
        },
        "id": "85Gl05AddYVY",
        "outputId": "3fbb5ef2-fbc0-4a8c-d411-76ccf71e9b40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1000])\n"
          ]
        }
      ],
      "source": [
        "feats = torch.rand(4, 80, 301)\n",
        "model = ECAPA_TDNN()\n",
        "embd = model(feats)\n",
        "\n",
        "classifier = AAMSoftmax(192, 1000, device_id=None, s=30.0, m=0.50)\n",
        "output = classifier(embd, torch.tensor([0, 1, 2, 3]))\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hJbmP3KAdYVY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "无",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "188.695663px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}